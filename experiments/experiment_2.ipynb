{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c03ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e70cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd7e7070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modi promised “minimum government maximum gove...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say vote modi welcome bjp told rahul main camp...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking supporter prefix chowkidar name modi gr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  modi promised “minimum government maximum gove...      -1.0\n",
       "1             talk nonsense continue drama vote modi       0.0\n",
       "2  say vote modi welcome bjp told rahul main camp...       1.0\n",
       "3  asking supporter prefix chowkidar name modi gr...       1.0\n",
       "4  answer among powerful world leader today trump...       1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../preprocessing_eda/data/processed/Preprocessed_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e7c5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['clean_text', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a4ab20",
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Cannot set a deleted experiment 'Pytorch LSTM' as the active experiment. You can restore the experiment, or permanently delete the experiment to create a new one.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPytorch LSTM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT\\Desktop\\CS\\Projects\\youtube-sentiment-analyser\\.venv\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:209\u001b[39m, in \u001b[36mset_experiment\u001b[39m\u001b[34m(experiment_name, experiment_id)\u001b[39m\n\u001b[32m    203\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    204\u001b[39m                 message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExperiment with ID \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    205\u001b[39m                 error_code=RESOURCE_DOES_NOT_EXIST,\n\u001b[32m    206\u001b[39m             )\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m experiment.lifecycle_stage != LifecycleStage.ACTIVE:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    210\u001b[39m             message=(\n\u001b[32m    211\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot set a deleted experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment.name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m as the active\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    212\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m experiment. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mYou can restore the experiment, or permanently delete the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    214\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mexperiment to create a new one.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m             ),\n\u001b[32m    216\u001b[39m             error_code=INVALID_PARAMETER_VALUE,\n\u001b[32m    217\u001b[39m         )\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _active_experiment_id\n\u001b[32m    220\u001b[39m _active_experiment_id = experiment.experiment_id\n",
      "\u001b[31mMlflowException\u001b[39m: Cannot set a deleted experiment 'Pytorch LSTM' as the active experiment. You can restore the experiment, or permanently delete the experiment to create a new one."
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Pytorch LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c16b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize and pad/truncate\n",
    "        tokens = self.tokenizer.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens = tokens + [0] * (self.max_length - len(tokens))\n",
    "        \n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84145cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tokenizer class\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, max_features=10000):\n",
    "        self.max_features = max_features\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Create vocabulary with most common words\n",
    "        most_common = word_counts.most_common(self.max_features - 1)\n",
    "        \n",
    "        # Reserve index 0 for padding\n",
    "        self.word_to_index = {'<PAD>': 0}\n",
    "        self.index_to_word = {0: '<PAD>'}\n",
    "        \n",
    "        for i, (word, _) in enumerate(most_common, 1):\n",
    "            self.word_to_index[word] = i\n",
    "            self.index_to_word[i] = word\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            sequence = [self.word_to_index.get(word, 0) for word in words]\n",
    "            sequences.append(sequence)\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685ba726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, dropout_rate, bidirectional=True):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate input dimension for fully connected layer\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(lstm_output_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last hidden state (for classification)\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward hidden states\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        output = self.dropout(hidden)\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6f95a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_pytorch_lstm(hidden_units, dropout_rate, num_layers=1, bidirectional=True):\n",
    "    \"\"\"\n",
    "    Run PyTorch LSTM experiment with varying architecture parameters\n",
    "    \n",
    "    Args:\n",
    "        hidden_units (int): Number of LSTM hidden units\n",
    "        dropout_rate (float): Dropout rate for regularization\n",
    "        num_layers (int): Number of LSTM layers\n",
    "        bidirectional (bool): Whether to use bidirectional LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Tokenization parameters\n",
    "    max_features = 10000  # Maximum vocabulary size\n",
    "    maxlen = 100  # Maximum sequence length\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['clean_text'], df['category'], \n",
    "        test_size=0.2, random_state=42, stratify=df['category']\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokenizer = SimpleTokenizer(max_features=max_features)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(X_train, y_train_encoded, tokenizer, maxlen)\n",
    "    val_dataset = TextDataset(X_val, y_val_encoded, tokenizer, maxlen)\n",
    "    test_dataset = TextDataset(X_test, y_test_encoded, tokenizer, maxlen)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        # Set tags for the experiment and run\n",
    "        lstm_type = \"BiLSTM\" if bidirectional else \"LSTM\"\n",
    "        mlflow.set_tag(\"mlflow.runName\", f\"PyTorch_{lstm_type}_units_{hidden_units}_layers_{num_layers}_dropout_{dropout_rate}\")\n",
    "        mlflow.set_tag(\"experiment_type\", \"pytorch_neural_network\")\n",
    "        mlflow.set_tag(\"model_type\", f\"PyTorch{lstm_type}Classifier\")\n",
    "        \n",
    "        # Add description\n",
    "        mlflow.set_tag(\"description\", \n",
    "                      f\"PyTorch {lstm_type} with {hidden_units} units, {num_layers} layers, dropout={dropout_rate}\")\n",
    "        \n",
    "        # Log preprocessing parameters\n",
    "        mlflow.log_param(\"tokenizer_max_features\", max_features)\n",
    "        mlflow.log_param(\"sequence_maxlen\", maxlen)\n",
    "        mlflow.log_param(\"vocab_size\", len(tokenizer.word_to_index))\n",
    "        \n",
    "        # Log model architecture parameters\n",
    "        mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "        mlflow.log_param(\"lstm_units\", hidden_units)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "        mlflow.log_param(\"bidirectional\", bidirectional)\n",
    "        mlflow.log_param(\"num_classes\", num_classes)\n",
    "        mlflow.log_param(\"device\", str(device))\n",
    "        \n",
    "        # Log training parameters\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SentimentLSTM(\n",
    "            vocab_size=len(tokenizer.word_to_index),\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_units,\n",
    "            num_layers=num_layers,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            bidirectional=bidirectional\n",
    "        ).to(device)\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Training history\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        best_val_accuracy = 0\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_texts, batch_labels in train_loader:\n",
    "                batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_texts)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += batch_labels.size(0)\n",
    "                train_correct += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_texts, batch_labels in val_loader:\n",
    "                    batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                    outputs = model(batch_texts)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += batch_labels.size(0)\n",
    "                    val_correct += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_loss_avg = train_loss / len(train_loader)\n",
    "            train_acc = train_correct / train_total\n",
    "            val_loss_avg = val_loss / len(val_loader)\n",
    "            val_acc = val_correct / val_total\n",
    "            \n",
    "            train_losses.append(train_loss_avg)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(val_loss_avg)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = val_acc\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(torch.load('best_model.pth'))\n",
    "        \n",
    "        # Log training metrics\n",
    "        mlflow.log_metric(\"actual_epochs\", epoch + 1)\n",
    "        mlflow.log_metric(\"final_train_loss\", train_losses[-1])\n",
    "        mlflow.log_metric(\"final_val_loss\", val_losses[-1])\n",
    "        mlflow.log_metric(\"final_train_accuracy\", train_accuracies[-1])\n",
    "        mlflow.log_metric(\"final_val_accuracy\", val_accuracies[-1])\n",
    "        mlflow.log_metric(\"best_val_accuracy\", best_val_accuracy)\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"training_history.png\")\n",
    "        mlflow.log_artifact(\"training_history.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_texts, batch_labels in test_loader:\n",
    "                batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_texts)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate test accuracy\n",
    "        test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        \n",
    "        # Log detailed classification metrics\n",
    "        classification_rep = classification_report(\n",
    "            all_labels, all_predictions, \n",
    "            target_names=label_encoder.classes_, \n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        for label, metrics in classification_rep.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    mlflow.log_metric(f\"{label}_{metric}\", value)\n",
    "        \n",
    "        # Create and log confusion matrix\n",
    "        conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                   xticklabels=label_encoder.classes_, \n",
    "                   yticklabels=label_encoder.classes_)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(f\"Confusion Matrix: PyTorch {lstm_type}, units={hidden_units}, layers={num_layers}\")\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.pytorch.log_model(\n",
    "            model, \n",
    "            f\"pytorch_lstm_model_units_{hidden_units}_layers_{num_layers}\",\n",
    "            registered_model_name=None\n",
    "        )\n",
    "        \n",
    "        # Log tokenizer and label encoder for later use\n",
    "        with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "        with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "            \n",
    "        mlflow.log_artifact(\"tokenizer.pkl\")\n",
    "        mlflow.log_artifact(\"label_encoder.pkl\")\n",
    "        \n",
    "        print(f\"Completed: PyTorch {lstm_type} with {hidden_units} units, {num_layers} layers, \"\n",
    "              f\"dropout={dropout_rate}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5892bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/30 03:08:55 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/30 03:09:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: PyTorch LSTM with 64 units, 1 layers, dropout=0.3, Test Accuracy: 0.8931\n",
      "🏃 View run PyTorch_LSTM_units_64_layers_1_dropout_0.3 at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/0/runs/80ab7f8140af4f6f8eae450df48f492d\n",
      "🧪 View experiment at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/0\n",
      "Using device: cpu\n",
      "🏃 View run PyTorch_LSTM_units_128_layers_1_dropout_0.3 at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/0/runs/b0e2efc52533425cb9b853814779f95a\n",
      "🧪 View experiment at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pytorch_configs:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mrun_experiment_pytorch_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mrun_experiment_pytorch_lstm\u001b[39m\u001b[34m(hidden_units, dropout_rate, num_layers, bidirectional)\u001b[39m\n\u001b[32m    128\u001b[39m loss = criterion(outputs, batch_labels)\n\u001b[32m    129\u001b[39m loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m train_loss += loss.item()\n\u001b[32m    133\u001b[39m _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT\\Desktop\\CS\\Projects\\youtube-sentiment-analyser\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT\\Desktop\\CS\\Projects\\youtube-sentiment-analyser\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT\\Desktop\\CS\\Projects\\youtube-sentiment-analyser\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT\\Desktop\\CS\\Projects\\youtube-sentiment-analyser\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT\\Desktop\\CS\\Projects\\youtube-sentiment-analyser\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KIIT\\Desktop\\CS\\Projects\\youtube-sentiment-analyser\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:533\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    531\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m         denom = (\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / bias_correction2_sqrt).add_(eps)\n\u001b[32m    535\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Experiment configurations\n",
    "pytorch_configs = [\n",
    "    {\"hidden_units\": 64, \"dropout_rate\": 0.3, \"num_layers\": 1, \"bidirectional\": False},\n",
    "    {\"hidden_units\": 128, \"dropout_rate\": 0.3, \"num_layers\": 1, \"bidirectional\": False},\n",
    "    {\"hidden_units\": 64, \"dropout_rate\": 0.5, \"num_layers\": 1, \"bidirectional\": True},\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "for config in pytorch_configs:\n",
    "    run_experiment_pytorch_lstm(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206100b",
   "metadata": {},
   "source": [
    "### 1 model is enough its taking way too long on my cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b6aeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow-bucket-241103/375501789462796807', creation_time=1756507494887, experiment_id='375501789462796807', last_update_time=1756507494887, lifecycle_stage='active', name='Pytorch CPU optimized LSTM', tags={}>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Pytorch CPU optimized LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ff9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment: 128 units, dropout=0.3, bidirectional=False\n",
      "Epoch 1/10, Batch 0/1997, Loss: 1.1058\n",
      "Epoch 1/10, Batch 50/1997, Loss: 1.0154\n",
      "Epoch 1/10, Batch 100/1997, Loss: 1.0868\n",
      "Epoch 1/10, Batch 150/1997, Loss: 1.1036\n",
      "Epoch 1/10, Batch 200/1997, Loss: 1.0704\n",
      "Epoch 1/10, Batch 250/1997, Loss: 1.0474\n",
      "Epoch 1/10, Batch 300/1997, Loss: 1.0135\n",
      "Epoch 1/10, Batch 350/1997, Loss: 1.1280\n",
      "Epoch 1/10, Batch 400/1997, Loss: 1.0494\n",
      "Epoch 1/10, Batch 450/1997, Loss: 1.0382\n",
      "Epoch 1/10, Batch 500/1997, Loss: 1.0894\n",
      "Epoch 1/10, Batch 550/1997, Loss: 1.0255\n",
      "Epoch 1/10, Batch 600/1997, Loss: 1.0379\n",
      "Epoch 1/10, Batch 650/1997, Loss: 0.9841\n",
      "Epoch 1/10, Batch 700/1997, Loss: 0.9188\n",
      "Epoch 1/10, Batch 750/1997, Loss: 0.9340\n",
      "Epoch 1/10, Batch 800/1997, Loss: 0.9356\n",
      "Epoch 1/10, Batch 850/1997, Loss: 1.0324\n",
      "Epoch 1/10, Batch 900/1997, Loss: 0.8799\n",
      "Epoch 1/10, Batch 950/1997, Loss: 0.7816\n",
      "Epoch 1/10, Batch 1000/1997, Loss: 0.7957\n",
      "Epoch 1/10, Batch 1050/1997, Loss: 0.7797\n",
      "Epoch 1/10, Batch 1100/1997, Loss: 0.6776\n",
      "Epoch 1/10, Batch 1150/1997, Loss: 0.7174\n",
      "Epoch 1/10, Batch 1200/1997, Loss: 0.5017\n",
      "Epoch 1/10, Batch 1250/1997, Loss: 0.7473\n",
      "Epoch 1/10, Batch 1300/1997, Loss: 0.6610\n",
      "Epoch 1/10, Batch 1350/1997, Loss: 0.6523\n",
      "Epoch 1/10, Batch 1400/1997, Loss: 0.4751\n",
      "Epoch 1/10, Batch 1450/1997, Loss: 0.7259\n",
      "Epoch 1/10, Batch 1500/1997, Loss: 0.6652\n",
      "Epoch 1/10, Batch 1550/1997, Loss: 0.4821\n",
      "Epoch 1/10, Batch 1600/1997, Loss: 0.6348\n",
      "Epoch 1/10, Batch 1650/1997, Loss: 0.5527\n",
      "Epoch 1/10, Batch 1700/1997, Loss: 0.4477\n",
      "Epoch 1/10, Batch 1750/1997, Loss: 0.4872\n",
      "Epoch 1/10, Batch 1800/1997, Loss: 0.5173\n",
      "Epoch 1/10, Batch 1850/1997, Loss: 0.6231\n",
      "Epoch 1/10, Batch 1900/1997, Loss: 0.5455\n",
      "Epoch 1/10, Batch 1950/1997, Loss: 0.5293\n",
      "Epoch 1: Train Loss: 0.8114, Val Acc: 0.7954\n",
      "Epoch 2/10, Batch 0/1997, Loss: 0.5476\n",
      "Epoch 2/10, Batch 50/1997, Loss: 0.4903\n",
      "Epoch 2/10, Batch 100/1997, Loss: 0.4274\n",
      "Epoch 2/10, Batch 150/1997, Loss: 0.4870\n",
      "Epoch 2/10, Batch 200/1997, Loss: 0.3889\n",
      "Epoch 2/10, Batch 250/1997, Loss: 0.4602\n",
      "Epoch 2/10, Batch 300/1997, Loss: 0.4179\n",
      "Epoch 2/10, Batch 350/1997, Loss: 0.4194\n",
      "Epoch 2/10, Batch 400/1997, Loss: 0.5387\n",
      "Epoch 2/10, Batch 450/1997, Loss: 0.2897\n",
      "Epoch 2/10, Batch 500/1997, Loss: 0.4550\n",
      "Epoch 2/10, Batch 550/1997, Loss: 0.2845\n",
      "Epoch 2/10, Batch 600/1997, Loss: 0.5445\n",
      "Epoch 2/10, Batch 650/1997, Loss: 0.3049\n",
      "Epoch 2/10, Batch 700/1997, Loss: 0.4954\n",
      "Epoch 2/10, Batch 750/1997, Loss: 0.4494\n",
      "Epoch 2/10, Batch 800/1997, Loss: 0.4224\n",
      "Epoch 2/10, Batch 850/1997, Loss: 0.4773\n",
      "Epoch 2/10, Batch 900/1997, Loss: 0.4520\n",
      "Epoch 2/10, Batch 950/1997, Loss: 0.4753\n",
      "Epoch 2/10, Batch 1000/1997, Loss: 0.4648\n",
      "Epoch 2/10, Batch 1050/1997, Loss: 0.7575\n",
      "Epoch 2/10, Batch 1100/1997, Loss: 0.6226\n",
      "Epoch 2/10, Batch 1150/1997, Loss: 0.5346\n",
      "Epoch 2/10, Batch 1200/1997, Loss: 0.4067\n",
      "Epoch 2/10, Batch 1250/1997, Loss: 0.4056\n",
      "Epoch 2/10, Batch 1300/1997, Loss: 0.4906\n",
      "Epoch 2/10, Batch 1350/1997, Loss: 0.1874\n",
      "Epoch 2/10, Batch 1400/1997, Loss: 0.4623\n",
      "Epoch 2/10, Batch 1450/1997, Loss: 0.4581\n",
      "Epoch 2/10, Batch 1500/1997, Loss: 0.6602\n",
      "Epoch 2/10, Batch 1550/1997, Loss: 0.4962\n",
      "Epoch 2/10, Batch 1600/1997, Loss: 0.4263\n",
      "Epoch 2/10, Batch 1650/1997, Loss: 0.3942\n",
      "Epoch 2/10, Batch 1700/1997, Loss: 0.3450\n",
      "Epoch 2/10, Batch 1750/1997, Loss: 0.5488\n",
      "Epoch 2/10, Batch 1800/1997, Loss: 0.3895\n",
      "Epoch 2/10, Batch 1850/1997, Loss: 0.2863\n",
      "Epoch 2/10, Batch 1900/1997, Loss: 0.3603\n",
      "Epoch 2/10, Batch 1950/1997, Loss: 0.4679\n",
      "Epoch 2: Train Loss: 0.4523, Val Acc: 0.8634\n",
      "Epoch 3/10, Batch 0/1997, Loss: 0.4920\n",
      "Epoch 3/10, Batch 50/1997, Loss: 0.3415\n",
      "Epoch 3/10, Batch 100/1997, Loss: 0.2694\n",
      "Epoch 3/10, Batch 150/1997, Loss: 0.3851\n",
      "Epoch 3/10, Batch 200/1997, Loss: 0.3064\n",
      "Epoch 3/10, Batch 250/1997, Loss: 0.3141\n",
      "Epoch 3/10, Batch 300/1997, Loss: 0.4706\n",
      "Epoch 3/10, Batch 350/1997, Loss: 0.4943\n",
      "Epoch 3/10, Batch 400/1997, Loss: 0.3106\n",
      "Epoch 3/10, Batch 450/1997, Loss: 0.3735\n",
      "Epoch 3/10, Batch 500/1997, Loss: 0.4032\n",
      "Epoch 3/10, Batch 550/1997, Loss: 0.3312\n",
      "Epoch 3/10, Batch 600/1997, Loss: 0.2942\n",
      "Epoch 3/10, Batch 650/1997, Loss: 0.4173\n",
      "Epoch 3/10, Batch 700/1997, Loss: 0.1991\n",
      "Epoch 3/10, Batch 750/1997, Loss: 0.4954\n",
      "Epoch 3/10, Batch 800/1997, Loss: 0.4358\n",
      "Epoch 3/10, Batch 850/1997, Loss: 0.5918\n",
      "Epoch 3/10, Batch 900/1997, Loss: 0.2645\n",
      "Epoch 3/10, Batch 950/1997, Loss: 0.5440\n",
      "Epoch 3/10, Batch 1000/1997, Loss: 0.2642\n",
      "Epoch 3/10, Batch 1050/1997, Loss: 0.3023\n",
      "Epoch 3/10, Batch 1100/1997, Loss: 0.1078\n",
      "Epoch 3/10, Batch 1150/1997, Loss: 0.3450\n",
      "Epoch 3/10, Batch 1200/1997, Loss: 0.2040\n",
      "Epoch 3/10, Batch 1250/1997, Loss: 0.3799\n",
      "Epoch 3/10, Batch 1300/1997, Loss: 0.3390\n",
      "Epoch 3/10, Batch 1350/1997, Loss: 0.2591\n",
      "Epoch 3/10, Batch 1400/1997, Loss: 0.3457\n",
      "Epoch 3/10, Batch 1450/1997, Loss: 0.4465\n",
      "Epoch 3/10, Batch 1500/1997, Loss: 0.3352\n",
      "Epoch 3/10, Batch 1550/1997, Loss: 0.3770\n",
      "Epoch 3/10, Batch 1600/1997, Loss: 0.2500\n",
      "Epoch 3/10, Batch 1650/1997, Loss: 0.3266\n",
      "Epoch 3/10, Batch 1700/1997, Loss: 0.5363\n",
      "Epoch 3/10, Batch 1750/1997, Loss: 0.3611\n",
      "Epoch 3/10, Batch 1800/1997, Loss: 0.2013\n",
      "Epoch 3/10, Batch 1850/1997, Loss: 0.2226\n",
      "Epoch 3/10, Batch 1900/1997, Loss: 0.3508\n",
      "Epoch 3/10, Batch 1950/1997, Loss: 0.2914\n",
      "Epoch 3: Train Loss: 0.3763, Val Acc: 0.8815\n",
      "Epoch 4/10, Batch 0/1997, Loss: 0.3218\n",
      "Epoch 4/10, Batch 50/1997, Loss: 0.2783\n",
      "Epoch 4/10, Batch 100/1997, Loss: 0.3437\n",
      "Epoch 4/10, Batch 150/1997, Loss: 0.3496\n",
      "Epoch 4/10, Batch 200/1997, Loss: 0.2688\n",
      "Epoch 4/10, Batch 250/1997, Loss: 0.3747\n",
      "Epoch 4/10, Batch 300/1997, Loss: 0.3713\n",
      "Epoch 4/10, Batch 350/1997, Loss: 0.3283\n",
      "Epoch 4/10, Batch 400/1997, Loss: 0.3444\n",
      "Epoch 4/10, Batch 450/1997, Loss: 0.2384\n",
      "Epoch 4/10, Batch 500/1997, Loss: 0.3417\n",
      "Epoch 4/10, Batch 550/1997, Loss: 0.3474\n",
      "Epoch 4/10, Batch 600/1997, Loss: 0.2789\n",
      "Epoch 4/10, Batch 650/1997, Loss: 0.2351\n",
      "Epoch 4/10, Batch 700/1997, Loss: 0.3253\n",
      "Epoch 4/10, Batch 750/1997, Loss: 0.4561\n",
      "Epoch 4/10, Batch 800/1997, Loss: 0.3400\n",
      "Epoch 4/10, Batch 850/1997, Loss: 0.4625\n",
      "Epoch 4/10, Batch 900/1997, Loss: 0.4979\n",
      "Epoch 4/10, Batch 950/1997, Loss: 0.4185\n",
      "Epoch 4/10, Batch 1000/1997, Loss: 0.1830\n",
      "Epoch 4/10, Batch 1050/1997, Loss: 0.3522\n",
      "Epoch 4/10, Batch 1100/1997, Loss: 0.4496\n",
      "Epoch 4/10, Batch 1150/1997, Loss: 0.1867\n",
      "Epoch 4/10, Batch 1200/1997, Loss: 0.3097\n",
      "Epoch 4/10, Batch 1250/1997, Loss: 0.3129\n",
      "Epoch 4/10, Batch 1300/1997, Loss: 0.3229\n",
      "Epoch 4/10, Batch 1350/1997, Loss: 0.4105\n",
      "Epoch 4/10, Batch 1400/1997, Loss: 0.2532\n",
      "Epoch 4/10, Batch 1450/1997, Loss: 0.2483\n",
      "Epoch 4/10, Batch 1500/1997, Loss: 0.2562\n",
      "Epoch 4/10, Batch 1550/1997, Loss: 0.3293\n",
      "Epoch 4/10, Batch 1600/1997, Loss: 0.2959\n",
      "Epoch 4/10, Batch 1650/1997, Loss: 0.3653\n",
      "Epoch 4/10, Batch 1700/1997, Loss: 0.4277\n",
      "Epoch 4/10, Batch 1750/1997, Loss: 0.2555\n",
      "Epoch 4/10, Batch 1800/1997, Loss: 0.2990\n",
      "Epoch 4/10, Batch 1850/1997, Loss: 0.5119\n",
      "Epoch 4/10, Batch 1900/1997, Loss: 0.4471\n",
      "Epoch 4/10, Batch 1950/1997, Loss: 0.3227\n",
      "Epoch 4: Train Loss: 0.3359, Val Acc: 0.8852\n",
      "Epoch 5/10, Batch 0/1997, Loss: 0.3019\n",
      "Epoch 5/10, Batch 50/1997, Loss: 0.5682\n",
      "Epoch 5/10, Batch 100/1997, Loss: 0.2021\n",
      "Epoch 5/10, Batch 150/1997, Loss: 0.1903\n",
      "Epoch 5/10, Batch 200/1997, Loss: 0.2891\n",
      "Epoch 5/10, Batch 250/1997, Loss: 0.2285\n",
      "Epoch 5/10, Batch 300/1997, Loss: 0.2759\n",
      "Epoch 5/10, Batch 350/1997, Loss: 0.3143\n",
      "Epoch 5/10, Batch 400/1997, Loss: 0.3836\n",
      "Epoch 5/10, Batch 450/1997, Loss: 0.2056\n",
      "Epoch 5/10, Batch 500/1997, Loss: 0.1293\n",
      "Epoch 5/10, Batch 550/1997, Loss: 0.2607\n",
      "Epoch 5/10, Batch 600/1997, Loss: 0.2617\n",
      "Epoch 5/10, Batch 650/1997, Loss: 0.1215\n",
      "Epoch 5/10, Batch 700/1997, Loss: 0.2581\n",
      "Epoch 5/10, Batch 750/1997, Loss: 0.4390\n",
      "Epoch 5/10, Batch 800/1997, Loss: 0.2737\n",
      "Epoch 5/10, Batch 850/1997, Loss: 0.1792\n",
      "Epoch 5/10, Batch 900/1997, Loss: 0.2847\n",
      "Epoch 5/10, Batch 950/1997, Loss: 0.4000\n",
      "Epoch 5/10, Batch 1000/1997, Loss: 0.2877\n",
      "Epoch 5/10, Batch 1050/1997, Loss: 0.4142\n",
      "Epoch 5/10, Batch 1100/1997, Loss: 0.4640\n",
      "Epoch 5/10, Batch 1150/1997, Loss: 0.2779\n",
      "Epoch 5/10, Batch 1200/1997, Loss: 0.3344\n",
      "Epoch 5/10, Batch 1250/1997, Loss: 0.4565\n",
      "Epoch 5/10, Batch 1300/1997, Loss: 0.2568\n",
      "Epoch 5/10, Batch 1350/1997, Loss: 0.1931\n",
      "Epoch 5/10, Batch 1400/1997, Loss: 0.3020\n",
      "Epoch 5/10, Batch 1450/1997, Loss: 0.3766\n",
      "Epoch 5/10, Batch 1500/1997, Loss: 0.2226\n",
      "Epoch 5/10, Batch 1550/1997, Loss: 0.2575\n",
      "Epoch 5/10, Batch 1600/1997, Loss: 0.3714\n",
      "Epoch 5/10, Batch 1650/1997, Loss: 0.4379\n",
      "Epoch 5/10, Batch 1700/1997, Loss: 0.3015\n",
      "Epoch 5/10, Batch 1750/1997, Loss: 0.1313\n",
      "Epoch 5/10, Batch 1800/1997, Loss: 0.3839\n",
      "Epoch 5/10, Batch 1850/1997, Loss: 0.2833\n",
      "Epoch 5/10, Batch 1900/1997, Loss: 0.3878\n",
      "Epoch 5/10, Batch 1950/1997, Loss: 0.1981\n",
      "Epoch 5: Train Loss: 0.3079, Val Acc: 0.8881\n",
      "Epoch 6/10, Batch 0/1997, Loss: 0.2462\n",
      "Epoch 6/10, Batch 50/1997, Loss: 0.2452\n",
      "Epoch 6/10, Batch 100/1997, Loss: 0.3221\n",
      "Epoch 6/10, Batch 150/1997, Loss: 0.1774\n",
      "Epoch 6/10, Batch 200/1997, Loss: 0.2615\n",
      "Epoch 6/10, Batch 250/1997, Loss: 0.4734\n",
      "Epoch 6/10, Batch 300/1997, Loss: 0.2415\n",
      "Epoch 6/10, Batch 350/1997, Loss: 0.2443\n",
      "Epoch 6/10, Batch 400/1997, Loss: 0.2485\n",
      "Epoch 6/10, Batch 450/1997, Loss: 0.2473\n",
      "Epoch 6/10, Batch 500/1997, Loss: 0.3087\n",
      "Epoch 6/10, Batch 550/1997, Loss: 0.3483\n",
      "Epoch 6/10, Batch 600/1997, Loss: 0.3491\n",
      "Epoch 6/10, Batch 650/1997, Loss: 0.3114\n",
      "Epoch 6/10, Batch 700/1997, Loss: 0.2949\n",
      "Epoch 6/10, Batch 750/1997, Loss: 0.3932\n",
      "Epoch 6/10, Batch 800/1997, Loss: 0.3439\n",
      "Epoch 6/10, Batch 850/1997, Loss: 0.2820\n",
      "Epoch 6/10, Batch 900/1997, Loss: 0.2918\n",
      "Epoch 6/10, Batch 950/1997, Loss: 0.3242\n",
      "Epoch 6/10, Batch 1000/1997, Loss: 0.0976\n",
      "Epoch 6/10, Batch 1050/1997, Loss: 0.2958\n",
      "Epoch 6/10, Batch 1100/1997, Loss: 0.2536\n",
      "Epoch 6/10, Batch 1150/1997, Loss: 0.2731\n",
      "Epoch 6/10, Batch 1200/1997, Loss: 0.2189\n",
      "Epoch 6/10, Batch 1250/1997, Loss: 0.2383\n",
      "Epoch 6/10, Batch 1300/1997, Loss: 0.2138\n",
      "Epoch 6/10, Batch 1350/1997, Loss: 0.2645\n",
      "Epoch 6/10, Batch 1400/1997, Loss: 0.1809\n",
      "Epoch 6/10, Batch 1450/1997, Loss: 0.1515\n",
      "Epoch 6/10, Batch 1500/1997, Loss: 0.2493\n",
      "Epoch 6/10, Batch 1550/1997, Loss: 0.2978\n",
      "Epoch 6/10, Batch 1600/1997, Loss: 0.5530\n",
      "Epoch 6/10, Batch 1650/1997, Loss: 0.2907\n",
      "Epoch 6/10, Batch 1700/1997, Loss: 0.3590\n",
      "Epoch 6/10, Batch 1750/1997, Loss: 0.4272\n",
      "Epoch 6/10, Batch 1800/1997, Loss: 0.1971\n",
      "Epoch 6/10, Batch 1850/1997, Loss: 0.3233\n",
      "Epoch 6/10, Batch 1900/1997, Loss: 0.3077\n",
      "Epoch 6/10, Batch 1950/1997, Loss: 0.3944\n",
      "Epoch 6: Train Loss: 0.2839, Val Acc: 0.8898\n",
      "Epoch 7/10, Batch 0/1997, Loss: 0.1097\n",
      "Epoch 7/10, Batch 50/1997, Loss: 0.4223\n",
      "Epoch 7/10, Batch 100/1997, Loss: 0.2692\n",
      "Epoch 7/10, Batch 150/1997, Loss: 0.3637\n",
      "Epoch 7/10, Batch 200/1997, Loss: 0.1100\n",
      "Epoch 7/10, Batch 250/1997, Loss: 0.2433\n",
      "Epoch 7/10, Batch 300/1997, Loss: 0.2636\n",
      "Epoch 7/10, Batch 350/1997, Loss: 0.3338\n",
      "Epoch 7/10, Batch 400/1997, Loss: 0.1234\n",
      "Epoch 7/10, Batch 450/1997, Loss: 0.2367\n",
      "Epoch 7/10, Batch 500/1997, Loss: 0.0849\n",
      "Epoch 7/10, Batch 550/1997, Loss: 0.2615\n",
      "Epoch 7/10, Batch 600/1997, Loss: 0.3233\n",
      "Epoch 7/10, Batch 650/1997, Loss: 0.3955\n",
      "Epoch 7/10, Batch 700/1997, Loss: 0.1910\n",
      "Epoch 7/10, Batch 750/1997, Loss: 0.2780\n",
      "Epoch 7/10, Batch 800/1997, Loss: 0.2677\n",
      "Epoch 7/10, Batch 850/1997, Loss: 0.2222\n",
      "Epoch 7/10, Batch 900/1997, Loss: 0.1762\n",
      "Epoch 7/10, Batch 950/1997, Loss: 0.2379\n",
      "Epoch 7/10, Batch 1000/1997, Loss: 0.3349\n",
      "Epoch 7/10, Batch 1050/1997, Loss: 0.2286\n",
      "Epoch 7/10, Batch 1100/1997, Loss: 0.1503\n",
      "Epoch 7/10, Batch 1150/1997, Loss: 0.3463\n",
      "Epoch 7/10, Batch 1200/1997, Loss: 0.2287\n",
      "Epoch 7/10, Batch 1250/1997, Loss: 0.2262\n",
      "Epoch 7/10, Batch 1300/1997, Loss: 0.3153\n",
      "Epoch 7/10, Batch 1350/1997, Loss: 0.1445\n",
      "Epoch 7/10, Batch 1400/1997, Loss: 0.2159\n",
      "Epoch 7/10, Batch 1450/1997, Loss: 0.2517\n",
      "Epoch 7/10, Batch 1500/1997, Loss: 0.1838\n",
      "Epoch 7/10, Batch 1550/1997, Loss: 0.4510\n",
      "Epoch 7/10, Batch 1600/1997, Loss: 0.1704\n",
      "Epoch 7/10, Batch 1650/1997, Loss: 0.2270\n",
      "Epoch 7/10, Batch 1700/1997, Loss: 0.2241\n",
      "Epoch 7/10, Batch 1750/1997, Loss: 0.3141\n",
      "Epoch 7/10, Batch 1800/1997, Loss: 0.1634\n",
      "Epoch 7/10, Batch 1850/1997, Loss: 0.1067\n",
      "Epoch 7/10, Batch 1900/1997, Loss: 0.1166\n",
      "Epoch 7/10, Batch 1950/1997, Loss: 0.1649\n",
      "Epoch 7: Train Loss: 0.2588, Val Acc: 0.8883\n",
      "Epoch 8/10, Batch 0/1997, Loss: 0.3705\n",
      "Epoch 8/10, Batch 50/1997, Loss: 0.0801\n",
      "Epoch 8/10, Batch 100/1997, Loss: 0.2057\n",
      "Epoch 8/10, Batch 150/1997, Loss: 0.1434\n",
      "Epoch 8/10, Batch 200/1997, Loss: 0.2548\n",
      "Epoch 8/10, Batch 250/1997, Loss: 0.1031\n",
      "Epoch 8/10, Batch 300/1997, Loss: 0.2710\n",
      "Epoch 8/10, Batch 350/1997, Loss: 0.3208\n",
      "Epoch 8/10, Batch 400/1997, Loss: 0.1912\n",
      "Epoch 8/10, Batch 450/1997, Loss: 0.1676\n",
      "Epoch 8/10, Batch 500/1997, Loss: 0.0866\n",
      "Epoch 8/10, Batch 550/1997, Loss: 0.2323\n",
      "Epoch 8/10, Batch 600/1997, Loss: 0.4379\n",
      "Epoch 8/10, Batch 650/1997, Loss: 0.1446\n",
      "Epoch 8/10, Batch 700/1997, Loss: 0.1142\n",
      "Epoch 8/10, Batch 750/1997, Loss: 0.1264\n",
      "Epoch 8/10, Batch 800/1997, Loss: 0.2132\n",
      "Epoch 8/10, Batch 850/1997, Loss: 0.1848\n",
      "Epoch 8/10, Batch 900/1997, Loss: 0.2638\n",
      "Epoch 8/10, Batch 950/1997, Loss: 0.2604\n",
      "Epoch 8/10, Batch 1000/1997, Loss: 0.1680\n",
      "Epoch 8/10, Batch 1050/1997, Loss: 0.1081\n",
      "Epoch 8/10, Batch 1100/1997, Loss: 0.2056\n",
      "Epoch 8/10, Batch 1150/1997, Loss: 0.3146\n",
      "Epoch 8/10, Batch 1200/1997, Loss: 0.0936\n",
      "Epoch 8/10, Batch 1250/1997, Loss: 0.1600\n",
      "Epoch 8/10, Batch 1300/1997, Loss: 0.2158\n",
      "Epoch 8/10, Batch 1350/1997, Loss: 0.2054\n",
      "Epoch 8/10, Batch 1400/1997, Loss: 0.4224\n",
      "Epoch 8/10, Batch 1450/1997, Loss: 0.3421\n",
      "Epoch 8/10, Batch 1500/1997, Loss: 0.4182\n",
      "Epoch 8/10, Batch 1550/1997, Loss: 0.1994\n",
      "Epoch 8/10, Batch 1600/1997, Loss: 0.1269\n",
      "Epoch 8/10, Batch 1650/1997, Loss: 0.1098\n",
      "Epoch 8/10, Batch 1700/1997, Loss: 0.3944\n",
      "Epoch 8/10, Batch 1750/1997, Loss: 0.2432\n",
      "Epoch 8/10, Batch 1800/1997, Loss: 0.2749\n",
      "Epoch 8/10, Batch 1850/1997, Loss: 0.3422\n",
      "Epoch 8/10, Batch 1900/1997, Loss: 0.2122\n",
      "Epoch 8/10, Batch 1950/1997, Loss: 0.3143\n",
      "Epoch 8: Train Loss: 0.2371, Val Acc: 0.8880\n",
      "Epoch 9/10, Batch 0/1997, Loss: 0.2844\n",
      "Epoch 9/10, Batch 50/1997, Loss: 0.2163\n",
      "Epoch 9/10, Batch 100/1997, Loss: 0.1763\n",
      "Epoch 9/10, Batch 150/1997, Loss: 0.2270\n",
      "Epoch 9/10, Batch 200/1997, Loss: 0.2111\n",
      "Epoch 9/10, Batch 250/1997, Loss: 0.0830\n",
      "Epoch 9/10, Batch 300/1997, Loss: 0.2793\n",
      "Epoch 9/10, Batch 350/1997, Loss: 0.1172\n",
      "Epoch 9/10, Batch 400/1997, Loss: 0.3158\n",
      "Epoch 9/10, Batch 450/1997, Loss: 0.2078\n",
      "Epoch 9/10, Batch 500/1997, Loss: 0.3364\n",
      "Epoch 9/10, Batch 550/1997, Loss: 0.1838\n",
      "Epoch 9/10, Batch 600/1997, Loss: 0.0815\n",
      "Epoch 9/10, Batch 650/1997, Loss: 0.2606\n",
      "Epoch 9/10, Batch 700/1997, Loss: 0.2073\n",
      "Epoch 9/10, Batch 750/1997, Loss: 0.4648\n",
      "Epoch 9/10, Batch 800/1997, Loss: 0.0680\n",
      "Epoch 9/10, Batch 850/1997, Loss: 0.1757\n",
      "Epoch 9/10, Batch 900/1997, Loss: 0.1406\n",
      "Epoch 9/10, Batch 950/1997, Loss: 0.1690\n",
      "Epoch 9/10, Batch 1000/1997, Loss: 0.3384\n",
      "Epoch 9/10, Batch 1050/1997, Loss: 0.1105\n",
      "Epoch 9/10, Batch 1100/1997, Loss: 0.2743\n",
      "Epoch 9/10, Batch 1150/1997, Loss: 0.3095\n",
      "Epoch 9/10, Batch 1200/1997, Loss: 0.1778\n",
      "Epoch 9/10, Batch 1250/1997, Loss: 0.2509\n",
      "Epoch 9/10, Batch 1300/1997, Loss: 0.1734\n",
      "Epoch 9/10, Batch 1350/1997, Loss: 0.2660\n",
      "Epoch 9/10, Batch 1400/1997, Loss: 0.1793\n",
      "Epoch 9/10, Batch 1450/1997, Loss: 0.1578\n",
      "Epoch 9/10, Batch 1500/1997, Loss: 0.2077\n",
      "Epoch 9/10, Batch 1550/1997, Loss: 0.2947\n",
      "Epoch 9/10, Batch 1600/1997, Loss: 0.3698\n",
      "Epoch 9/10, Batch 1650/1997, Loss: 0.3109\n",
      "Epoch 9/10, Batch 1700/1997, Loss: 0.1965\n",
      "Epoch 9/10, Batch 1750/1997, Loss: 0.3168\n",
      "Epoch 9/10, Batch 1800/1997, Loss: 0.1746\n",
      "Epoch 9/10, Batch 1850/1997, Loss: 0.1357\n",
      "Epoch 9/10, Batch 1900/1997, Loss: 0.2955\n",
      "Epoch 9/10, Batch 1950/1997, Loss: 0.2081\n",
      "Epoch 9: Train Loss: 0.2148, Val Acc: 0.8865\n",
      "Epoch 10/10, Batch 0/1997, Loss: 0.1462\n",
      "Epoch 10/10, Batch 50/1997, Loss: 0.1606\n",
      "Epoch 10/10, Batch 100/1997, Loss: 0.0669\n",
      "Epoch 10/10, Batch 150/1997, Loss: 0.3347\n",
      "Epoch 10/10, Batch 200/1997, Loss: 0.2341\n",
      "Epoch 10/10, Batch 250/1997, Loss: 0.3490\n",
      "Epoch 10/10, Batch 300/1997, Loss: 0.0863\n",
      "Epoch 10/10, Batch 350/1997, Loss: 0.0783\n",
      "Epoch 10/10, Batch 400/1997, Loss: 0.3991\n",
      "Epoch 10/10, Batch 450/1997, Loss: 0.3051\n",
      "Epoch 10/10, Batch 500/1997, Loss: 0.1666\n",
      "Epoch 10/10, Batch 550/1997, Loss: 0.1662\n",
      "Epoch 10/10, Batch 600/1997, Loss: 0.1872\n",
      "Epoch 10/10, Batch 650/1997, Loss: 0.1972\n",
      "Epoch 10/10, Batch 700/1997, Loss: 0.1693\n",
      "Epoch 10/10, Batch 750/1997, Loss: 0.1016\n",
      "Epoch 10/10, Batch 800/1997, Loss: 0.1026\n",
      "Epoch 10/10, Batch 850/1997, Loss: 0.2281\n",
      "Epoch 10/10, Batch 900/1997, Loss: 0.2456\n",
      "Epoch 10/10, Batch 950/1997, Loss: 0.1308\n",
      "Epoch 10/10, Batch 1000/1997, Loss: 0.2519\n",
      "Epoch 10/10, Batch 1050/1997, Loss: 0.1009\n",
      "Epoch 10/10, Batch 1100/1997, Loss: 0.0992\n",
      "Epoch 10/10, Batch 1150/1997, Loss: 0.1348\n",
      "Epoch 10/10, Batch 1200/1997, Loss: 0.2831\n",
      "Epoch 10/10, Batch 1250/1997, Loss: 0.1252\n",
      "Epoch 10/10, Batch 1300/1997, Loss: 0.1023\n",
      "Epoch 10/10, Batch 1350/1997, Loss: 0.0764\n",
      "Epoch 10/10, Batch 1400/1997, Loss: 0.0702\n",
      "Epoch 10/10, Batch 1450/1997, Loss: 0.0967\n",
      "Epoch 10/10, Batch 1500/1997, Loss: 0.1437\n",
      "Epoch 10/10, Batch 1550/1997, Loss: 0.2426\n",
      "Epoch 10/10, Batch 1600/1997, Loss: 0.1567\n",
      "Epoch 10/10, Batch 1650/1997, Loss: 0.1327\n",
      "Epoch 10/10, Batch 1700/1997, Loss: 0.1223\n",
      "Epoch 10/10, Batch 1750/1997, Loss: 0.1428\n",
      "Epoch 10/10, Batch 1800/1997, Loss: 0.1978\n",
      "Epoch 10/10, Batch 1850/1997, Loss: 0.2135\n",
      "Epoch 10/10, Batch 1900/1997, Loss: 0.2168\n",
      "Epoch 10/10, Batch 1950/1997, Loss: 0.2203\n",
      "Epoch 10: Train Loss: 0.1950, Val Acc: 0.8832\n",
      "Batch 1/624\n",
      "Batch 21/624\n",
      "Batch 41/624\n",
      "Batch 61/624\n",
      "Batch 81/624\n",
      "Batch 101/624\n",
      "Batch 121/624\n",
      "Batch 141/624\n",
      "Batch 161/624\n",
      "Batch 181/624\n",
      "Batch 201/624\n",
      "Batch 221/624\n",
      "Batch 241/624\n",
      "Batch 261/624\n",
      "Batch 281/624\n",
      "Batch 301/624\n",
      "Batch 321/624\n",
      "Batch 341/624\n",
      "Batch 361/624\n",
      "Batch 381/624\n",
      "Batch 401/624\n",
      "Batch 421/624\n",
      "Batch 441/624\n",
      "Batch 461/624\n",
      "Batch 481/624\n",
      "Batch 501/624\n",
      "Batch 521/624\n",
      "Batch 541/624\n",
      "Batch 561/624\n",
      "Batch 581/624\n",
      "Batch 601/624\n",
      "Batch 621/624\n",
      "🏃 View run CPU_Quick_LSTM_units_128_dropout_0.3 at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/375501789462796807/runs/7c2d5bdeaee1450ababf12b55739a7bc\n",
      "🧪 View experiment at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/375501789462796807\n",
      "Starting experiment: 64 units, dropout=0.5, bidirectional=True\n",
      "Epoch 1/10, Batch 0/1997, Loss: 1.0730\n",
      "Epoch 1/10, Batch 50/1997, Loss: 1.0593\n",
      "Epoch 1/10, Batch 100/1997, Loss: 1.0146\n",
      "Epoch 1/10, Batch 150/1997, Loss: 1.1101\n",
      "Epoch 1/10, Batch 200/1997, Loss: 0.9360\n",
      "Epoch 1/10, Batch 250/1997, Loss: 0.8224\n",
      "Epoch 1/10, Batch 300/1997, Loss: 0.8064\n",
      "Epoch 1/10, Batch 350/1997, Loss: 0.8813\n",
      "Epoch 1/10, Batch 400/1997, Loss: 0.7366\n",
      "Epoch 1/10, Batch 450/1997, Loss: 0.8446\n",
      "Epoch 1/10, Batch 500/1997, Loss: 0.8699\n",
      "Epoch 1/10, Batch 550/1997, Loss: 0.7234\n",
      "Epoch 1/10, Batch 600/1997, Loss: 0.7049\n",
      "Epoch 1/10, Batch 650/1997, Loss: 0.7129\n",
      "Epoch 1/10, Batch 700/1997, Loss: 1.0284\n",
      "Epoch 1/10, Batch 750/1997, Loss: 0.8300\n",
      "Epoch 1/10, Batch 800/1997, Loss: 0.7720\n",
      "Epoch 1/10, Batch 850/1997, Loss: 0.7151\n",
      "Epoch 1/10, Batch 900/1997, Loss: 0.7599\n",
      "Epoch 1/10, Batch 950/1997, Loss: 0.7438\n",
      "Epoch 1/10, Batch 1000/1997, Loss: 0.8344\n",
      "Epoch 1/10, Batch 1050/1997, Loss: 0.6555\n",
      "Epoch 1/10, Batch 1100/1997, Loss: 0.6236\n",
      "Epoch 1/10, Batch 1150/1997, Loss: 0.5477\n",
      "Epoch 1/10, Batch 1200/1997, Loss: 0.5545\n",
      "Epoch 1/10, Batch 1250/1997, Loss: 0.5898\n",
      "Epoch 1/10, Batch 1300/1997, Loss: 0.7455\n",
      "Epoch 1/10, Batch 1350/1997, Loss: 0.6431\n",
      "Epoch 1/10, Batch 1400/1997, Loss: 0.4727\n",
      "Epoch 1/10, Batch 1450/1997, Loss: 0.6255\n",
      "Epoch 1/10, Batch 1500/1997, Loss: 0.6152\n",
      "Epoch 1/10, Batch 1550/1997, Loss: 0.5409\n",
      "Epoch 1/10, Batch 1600/1997, Loss: 0.5342\n",
      "Epoch 1/10, Batch 1650/1997, Loss: 0.5826\n",
      "Epoch 1/10, Batch 1700/1997, Loss: 0.5391\n",
      "Epoch 1/10, Batch 1750/1997, Loss: 0.7086\n",
      "Epoch 1/10, Batch 1800/1997, Loss: 0.6145\n",
      "Epoch 1/10, Batch 1850/1997, Loss: 0.5086\n",
      "Epoch 1/10, Batch 1900/1997, Loss: 0.6909\n",
      "Epoch 1/10, Batch 1950/1997, Loss: 0.4452\n",
      "Epoch 1: Train Loss: 0.7320, Val Acc: 0.8227\n",
      "Epoch 2/10, Batch 0/1997, Loss: 0.5689\n",
      "Epoch 2/10, Batch 50/1997, Loss: 0.4247\n",
      "Epoch 2/10, Batch 100/1997, Loss: 0.4039\n",
      "Epoch 2/10, Batch 150/1997, Loss: 0.3111\n",
      "Epoch 2/10, Batch 200/1997, Loss: 0.5394\n",
      "Epoch 2/10, Batch 250/1997, Loss: 0.6252\n",
      "Epoch 2/10, Batch 300/1997, Loss: 0.4722\n",
      "Epoch 2/10, Batch 350/1997, Loss: 0.4958\n",
      "Epoch 2/10, Batch 400/1997, Loss: 0.3300\n",
      "Epoch 2/10, Batch 450/1997, Loss: 0.3552\n",
      "Epoch 2/10, Batch 500/1997, Loss: 0.4448\n",
      "Epoch 2/10, Batch 550/1997, Loss: 0.5523\n",
      "Epoch 2/10, Batch 600/1997, Loss: 0.8072\n",
      "Epoch 2/10, Batch 650/1997, Loss: 0.4987\n",
      "Epoch 2/10, Batch 700/1997, Loss: 0.5776\n",
      "Epoch 2/10, Batch 750/1997, Loss: 0.4988\n",
      "Epoch 2/10, Batch 800/1997, Loss: 0.4071\n",
      "Epoch 2/10, Batch 850/1997, Loss: 0.3442\n",
      "Epoch 2/10, Batch 900/1997, Loss: 0.3871\n",
      "Epoch 2/10, Batch 950/1997, Loss: 0.5357\n",
      "Epoch 2/10, Batch 1000/1997, Loss: 0.3915\n",
      "Epoch 2/10, Batch 1050/1997, Loss: 0.5891\n",
      "Epoch 2/10, Batch 1100/1997, Loss: 0.3974\n",
      "Epoch 2/10, Batch 1150/1997, Loss: 0.3206\n",
      "Epoch 2/10, Batch 1200/1997, Loss: 0.5391\n",
      "Epoch 2/10, Batch 1250/1997, Loss: 0.4516\n",
      "Epoch 2/10, Batch 1300/1997, Loss: 0.5076\n",
      "Epoch 2/10, Batch 1350/1997, Loss: 0.3920\n",
      "Epoch 2/10, Batch 1400/1997, Loss: 0.2528\n",
      "Epoch 2/10, Batch 1450/1997, Loss: 0.3143\n",
      "Epoch 2/10, Batch 1500/1997, Loss: 0.6031\n",
      "Epoch 2/10, Batch 1550/1997, Loss: 0.3816\n",
      "Epoch 2/10, Batch 1600/1997, Loss: 0.5073\n",
      "Epoch 2/10, Batch 1650/1997, Loss: 0.4272\n",
      "Epoch 2/10, Batch 1700/1997, Loss: 0.5561\n",
      "Epoch 2/10, Batch 1750/1997, Loss: 0.2642\n",
      "Epoch 2/10, Batch 1800/1997, Loss: 0.3091\n",
      "Epoch 2/10, Batch 1850/1997, Loss: 0.3484\n",
      "Epoch 2/10, Batch 1900/1997, Loss: 0.3261\n",
      "Epoch 2/10, Batch 1950/1997, Loss: 0.4231\n",
      "Epoch 2: Train Loss: 0.4406, Val Acc: 0.8736\n",
      "Epoch 3/10, Batch 0/1997, Loss: 0.5174\n",
      "Epoch 3/10, Batch 50/1997, Loss: 0.2880\n",
      "Epoch 3/10, Batch 100/1997, Loss: 0.3160\n",
      "Epoch 3/10, Batch 150/1997, Loss: 0.5396\n",
      "Epoch 3/10, Batch 200/1997, Loss: 0.3052\n",
      "Epoch 3/10, Batch 250/1997, Loss: 0.2891\n",
      "Epoch 3/10, Batch 300/1997, Loss: 0.2951\n",
      "Epoch 3/10, Batch 350/1997, Loss: 0.3679\n",
      "Epoch 3/10, Batch 400/1997, Loss: 0.3772\n",
      "Epoch 3/10, Batch 450/1997, Loss: 0.3872\n",
      "Epoch 3/10, Batch 500/1997, Loss: 0.2692\n",
      "Epoch 3/10, Batch 550/1997, Loss: 0.3876\n",
      "Epoch 3/10, Batch 600/1997, Loss: 0.1456\n",
      "Epoch 3/10, Batch 650/1997, Loss: 0.3776\n",
      "Epoch 3/10, Batch 700/1997, Loss: 0.2254\n",
      "Epoch 3/10, Batch 750/1997, Loss: 0.2063\n",
      "Epoch 3/10, Batch 800/1997, Loss: 0.2010\n",
      "Epoch 3/10, Batch 850/1997, Loss: 0.4305\n",
      "Epoch 3/10, Batch 900/1997, Loss: 0.3604\n",
      "Epoch 3/10, Batch 950/1997, Loss: 0.3993\n",
      "Epoch 3/10, Batch 1000/1997, Loss: 0.3681\n",
      "Epoch 3/10, Batch 1050/1997, Loss: 0.2918\n",
      "Epoch 3/10, Batch 1100/1997, Loss: 0.6118\n",
      "Epoch 3/10, Batch 1150/1997, Loss: 0.3755\n",
      "Epoch 3/10, Batch 1200/1997, Loss: 0.2998\n",
      "Epoch 3/10, Batch 1250/1997, Loss: 0.3609\n",
      "Epoch 3/10, Batch 1300/1997, Loss: 0.3717\n",
      "Epoch 3/10, Batch 1350/1997, Loss: 0.4987\n",
      "Epoch 3/10, Batch 1400/1997, Loss: 0.3031\n",
      "Epoch 3/10, Batch 1450/1997, Loss: 0.4702\n",
      "Epoch 3/10, Batch 1500/1997, Loss: 0.5677\n",
      "Epoch 3/10, Batch 1550/1997, Loss: 0.2391\n",
      "Epoch 3/10, Batch 1600/1997, Loss: 0.4622\n",
      "Epoch 3/10, Batch 1650/1997, Loss: 0.1512\n",
      "Epoch 3/10, Batch 1700/1997, Loss: 0.2549\n",
      "Epoch 3/10, Batch 1750/1997, Loss: 0.4180\n",
      "Epoch 3/10, Batch 1800/1997, Loss: 0.4499\n",
      "Epoch 3/10, Batch 1850/1997, Loss: 0.5689\n",
      "Epoch 3/10, Batch 1900/1997, Loss: 0.4021\n",
      "Epoch 3/10, Batch 1950/1997, Loss: 0.3206\n",
      "Epoch 3: Train Loss: 0.3719, Val Acc: 0.8851\n",
      "Epoch 4/10, Batch 0/1997, Loss: 0.3527\n",
      "Epoch 4/10, Batch 50/1997, Loss: 0.4052\n",
      "Epoch 4/10, Batch 100/1997, Loss: 0.3539\n",
      "Epoch 4/10, Batch 150/1997, Loss: 0.2547\n",
      "Epoch 4/10, Batch 200/1997, Loss: 0.4770\n",
      "Epoch 4/10, Batch 250/1997, Loss: 0.2452\n",
      "Epoch 4/10, Batch 300/1997, Loss: 0.4423\n",
      "Epoch 4/10, Batch 350/1997, Loss: 0.1919\n",
      "Epoch 4/10, Batch 400/1997, Loss: 0.4092\n",
      "Epoch 4/10, Batch 450/1997, Loss: 0.4428\n",
      "Epoch 4/10, Batch 500/1997, Loss: 0.3420\n",
      "Epoch 4/10, Batch 550/1997, Loss: 0.2657\n",
      "Epoch 4/10, Batch 600/1997, Loss: 0.6905\n",
      "Epoch 4/10, Batch 650/1997, Loss: 0.1876\n",
      "Epoch 4/10, Batch 700/1997, Loss: 0.5319\n",
      "Epoch 4/10, Batch 750/1997, Loss: 0.4744\n",
      "Epoch 4/10, Batch 800/1997, Loss: 0.2493\n",
      "Epoch 4/10, Batch 850/1997, Loss: 0.2616\n",
      "Epoch 4/10, Batch 900/1997, Loss: 0.4518\n",
      "Epoch 4/10, Batch 950/1997, Loss: 0.4263\n",
      "Epoch 4/10, Batch 1000/1997, Loss: 0.4391\n",
      "Epoch 4/10, Batch 1050/1997, Loss: 0.2537\n",
      "Epoch 4/10, Batch 1100/1997, Loss: 0.2297\n",
      "Epoch 4/10, Batch 1150/1997, Loss: 0.4479\n",
      "Epoch 4/10, Batch 1200/1997, Loss: 0.4108\n",
      "Epoch 4/10, Batch 1250/1997, Loss: 0.4527\n",
      "Epoch 4/10, Batch 1300/1997, Loss: 0.3334\n",
      "Epoch 4/10, Batch 1350/1997, Loss: 0.3806\n",
      "Epoch 4/10, Batch 1400/1997, Loss: 0.1932\n",
      "Epoch 4/10, Batch 1450/1997, Loss: 0.3745\n",
      "Epoch 4/10, Batch 1500/1997, Loss: 0.3670\n",
      "Epoch 4/10, Batch 1550/1997, Loss: 0.3216\n",
      "Epoch 4/10, Batch 1600/1997, Loss: 0.3715\n",
      "Epoch 4/10, Batch 1650/1997, Loss: 0.3568\n",
      "Epoch 4/10, Batch 1700/1997, Loss: 0.3640\n",
      "Epoch 4/10, Batch 1750/1997, Loss: 0.3283\n",
      "Epoch 4/10, Batch 1800/1997, Loss: 0.3430\n",
      "Epoch 4/10, Batch 1850/1997, Loss: 0.2182\n",
      "Epoch 4/10, Batch 1900/1997, Loss: 0.4648\n",
      "Epoch 4/10, Batch 1950/1997, Loss: 0.2978\n",
      "Epoch 4: Train Loss: 0.3424, Val Acc: 0.8881\n",
      "Epoch 5/10, Batch 0/1997, Loss: 0.1991\n",
      "Epoch 5/10, Batch 50/1997, Loss: 0.3319\n",
      "Epoch 5/10, Batch 100/1997, Loss: 0.3333\n",
      "Epoch 5/10, Batch 150/1997, Loss: 0.2520\n",
      "Epoch 5/10, Batch 200/1997, Loss: 0.2688\n",
      "Epoch 5/10, Batch 250/1997, Loss: 0.2467\n",
      "Epoch 5/10, Batch 300/1997, Loss: 0.4732\n",
      "Epoch 5/10, Batch 350/1997, Loss: 0.2005\n",
      "Epoch 5/10, Batch 400/1997, Loss: 0.3329\n",
      "Epoch 5/10, Batch 450/1997, Loss: 0.2558\n",
      "Epoch 5/10, Batch 500/1997, Loss: 0.1322\n",
      "Epoch 5/10, Batch 550/1997, Loss: 0.2172\n",
      "Epoch 5/10, Batch 600/1997, Loss: 0.5222\n",
      "Epoch 5/10, Batch 650/1997, Loss: 0.2037\n",
      "Epoch 5/10, Batch 700/1997, Loss: 0.3677\n",
      "Epoch 5/10, Batch 750/1997, Loss: 0.2289\n",
      "Epoch 5/10, Batch 800/1997, Loss: 0.2476\n",
      "Epoch 5/10, Batch 850/1997, Loss: 0.2262\n",
      "Epoch 5/10, Batch 900/1997, Loss: 0.2752\n",
      "Epoch 5/10, Batch 950/1997, Loss: 0.3037\n",
      "Epoch 5/10, Batch 1000/1997, Loss: 0.3860\n",
      "Epoch 5/10, Batch 1050/1997, Loss: 0.4637\n",
      "Epoch 5/10, Batch 1100/1997, Loss: 0.4132\n",
      "Epoch 5/10, Batch 1150/1997, Loss: 0.2597\n",
      "Epoch 5/10, Batch 1200/1997, Loss: 0.3096\n",
      "Epoch 5/10, Batch 1250/1997, Loss: 0.5875\n",
      "Epoch 5/10, Batch 1300/1997, Loss: 0.3544\n",
      "Epoch 5/10, Batch 1350/1997, Loss: 0.1924\n",
      "Epoch 5/10, Batch 1400/1997, Loss: 0.3415\n",
      "Epoch 5/10, Batch 1450/1997, Loss: 0.3364\n",
      "Epoch 5/10, Batch 1500/1997, Loss: 0.4503\n",
      "Epoch 5/10, Batch 1550/1997, Loss: 0.2054\n",
      "Epoch 5/10, Batch 1600/1997, Loss: 0.3793\n",
      "Epoch 5/10, Batch 1650/1997, Loss: 0.2930\n",
      "Epoch 5/10, Batch 1700/1997, Loss: 0.3201\n",
      "Epoch 5/10, Batch 1750/1997, Loss: 0.3653\n",
      "Epoch 5/10, Batch 1800/1997, Loss: 0.3382\n",
      "Epoch 5/10, Batch 1850/1997, Loss: 0.2883\n",
      "Epoch 5/10, Batch 1900/1997, Loss: 0.3823\n",
      "Epoch 5/10, Batch 1950/1997, Loss: 0.5477\n",
      "Epoch 5: Train Loss: 0.3182, Val Acc: 0.8920\n",
      "Epoch 6/10, Batch 0/1997, Loss: 0.2709\n",
      "Epoch 6/10, Batch 50/1997, Loss: 0.3926\n",
      "Epoch 6/10, Batch 100/1997, Loss: 0.3356\n",
      "Epoch 6/10, Batch 150/1997, Loss: 0.3312\n",
      "Epoch 6/10, Batch 200/1997, Loss: 0.3141\n",
      "Epoch 6/10, Batch 250/1997, Loss: 0.3604\n",
      "Epoch 6/10, Batch 300/1997, Loss: 0.4003\n",
      "Epoch 6/10, Batch 350/1997, Loss: 0.2358\n",
      "Epoch 6/10, Batch 400/1997, Loss: 0.4947\n",
      "Epoch 6/10, Batch 450/1997, Loss: 0.3042\n",
      "Epoch 6/10, Batch 500/1997, Loss: 0.2544\n",
      "Epoch 6/10, Batch 550/1997, Loss: 0.1352\n",
      "Epoch 6/10, Batch 600/1997, Loss: 0.3535\n",
      "Epoch 6/10, Batch 650/1997, Loss: 0.2732\n",
      "Epoch 6/10, Batch 700/1997, Loss: 0.1687\n",
      "Epoch 6/10, Batch 750/1997, Loss: 0.4044\n",
      "Epoch 6/10, Batch 800/1997, Loss: 0.2825\n",
      "Epoch 6/10, Batch 850/1997, Loss: 0.4840\n",
      "Epoch 6/10, Batch 900/1997, Loss: 0.3044\n",
      "Epoch 6/10, Batch 950/1997, Loss: 0.4086\n",
      "Epoch 6/10, Batch 1000/1997, Loss: 0.3079\n",
      "Epoch 6/10, Batch 1050/1997, Loss: 0.2618\n",
      "Epoch 6/10, Batch 1100/1997, Loss: 0.5562\n",
      "Epoch 6/10, Batch 1150/1997, Loss: 0.2071\n",
      "Epoch 6/10, Batch 1200/1997, Loss: 0.3769\n",
      "Epoch 6/10, Batch 1250/1997, Loss: 0.3368\n",
      "Epoch 6/10, Batch 1300/1997, Loss: 0.3525\n",
      "Epoch 6/10, Batch 1350/1997, Loss: 0.1617\n",
      "Epoch 6/10, Batch 1400/1997, Loss: 0.3249\n",
      "Epoch 6/10, Batch 1450/1997, Loss: 0.3422\n",
      "Epoch 6/10, Batch 1500/1997, Loss: 0.3185\n",
      "Epoch 6/10, Batch 1550/1997, Loss: 0.3991\n",
      "Epoch 6/10, Batch 1600/1997, Loss: 0.4721\n",
      "Epoch 6/10, Batch 1650/1997, Loss: 0.2353\n",
      "Epoch 6/10, Batch 1700/1997, Loss: 0.2077\n",
      "Epoch 6/10, Batch 1750/1997, Loss: 0.1867\n",
      "Epoch 6/10, Batch 1800/1997, Loss: 0.3359\n",
      "Epoch 6/10, Batch 1850/1997, Loss: 0.4584\n",
      "Epoch 6/10, Batch 1900/1997, Loss: 0.3600\n",
      "Epoch 6/10, Batch 1950/1997, Loss: 0.2891\n",
      "Epoch 6: Train Loss: 0.3016, Val Acc: 0.8911\n",
      "Epoch 7/10, Batch 0/1997, Loss: 0.3205\n",
      "Epoch 7/10, Batch 50/1997, Loss: 0.2771\n",
      "Epoch 7/10, Batch 100/1997, Loss: 0.1601\n",
      "Epoch 7/10, Batch 150/1997, Loss: 0.1941\n",
      "Epoch 7/10, Batch 200/1997, Loss: 0.2581\n",
      "Epoch 7/10, Batch 250/1997, Loss: 0.4577\n",
      "Epoch 7/10, Batch 300/1997, Loss: 0.1408\n",
      "Epoch 7/10, Batch 350/1997, Loss: 0.2777\n",
      "Epoch 7/10, Batch 400/1997, Loss: 0.2330\n",
      "Epoch 7/10, Batch 450/1997, Loss: 0.3405\n",
      "Epoch 7/10, Batch 500/1997, Loss: 0.2634\n",
      "Epoch 7/10, Batch 550/1997, Loss: 0.3750\n",
      "Epoch 7/10, Batch 600/1997, Loss: 0.2636\n",
      "Epoch 7/10, Batch 650/1997, Loss: 0.2865\n",
      "Epoch 7/10, Batch 700/1997, Loss: 0.2566\n",
      "Epoch 7/10, Batch 750/1997, Loss: 0.4195\n",
      "Epoch 7/10, Batch 800/1997, Loss: 0.1095\n",
      "Epoch 7/10, Batch 850/1997, Loss: 0.4954\n",
      "Epoch 7/10, Batch 900/1997, Loss: 0.2527\n",
      "Epoch 7/10, Batch 950/1997, Loss: 0.3133\n",
      "Epoch 7/10, Batch 1000/1997, Loss: 0.2116\n",
      "Epoch 7/10, Batch 1050/1997, Loss: 0.2018\n",
      "Epoch 7/10, Batch 1100/1997, Loss: 0.4840\n",
      "Epoch 7/10, Batch 1150/1997, Loss: 0.2795\n",
      "Epoch 7/10, Batch 1200/1997, Loss: 0.3288\n",
      "Epoch 7/10, Batch 1250/1997, Loss: 0.2684\n",
      "Epoch 7/10, Batch 1300/1997, Loss: 0.2290\n",
      "Epoch 7/10, Batch 1350/1997, Loss: 0.1856\n",
      "Epoch 7/10, Batch 1400/1997, Loss: 0.2835\n",
      "Epoch 7/10, Batch 1450/1997, Loss: 0.2566\n",
      "Epoch 7/10, Batch 1500/1997, Loss: 0.1944\n",
      "Epoch 7/10, Batch 1550/1997, Loss: 0.2391\n",
      "Epoch 7/10, Batch 1600/1997, Loss: 0.5531\n",
      "Epoch 7/10, Batch 1650/1997, Loss: 0.2954\n",
      "Epoch 7/10, Batch 1700/1997, Loss: 0.2212\n",
      "Epoch 7/10, Batch 1750/1997, Loss: 0.2808\n",
      "Epoch 7/10, Batch 1800/1997, Loss: 0.2832\n",
      "Epoch 7/10, Batch 1850/1997, Loss: 0.1944\n",
      "Epoch 7/10, Batch 1900/1997, Loss: 0.2984\n",
      "Epoch 7/10, Batch 1950/1997, Loss: 0.2509\n",
      "Epoch 7: Train Loss: 0.2862, Val Acc: 0.8893\n",
      "Epoch 8/10, Batch 0/1997, Loss: 0.3956\n",
      "Epoch 8/10, Batch 50/1997, Loss: 0.1487\n",
      "Epoch 8/10, Batch 100/1997, Loss: 0.1591\n",
      "Epoch 8/10, Batch 150/1997, Loss: 0.3220\n",
      "Epoch 8/10, Batch 200/1997, Loss: 0.4735\n",
      "Epoch 8/10, Batch 250/1997, Loss: 0.3056\n",
      "Epoch 8/10, Batch 300/1997, Loss: 0.4320\n",
      "Epoch 8/10, Batch 350/1997, Loss: 0.2034\n",
      "Epoch 8/10, Batch 400/1997, Loss: 0.1141\n",
      "Epoch 8/10, Batch 450/1997, Loss: 0.1591\n",
      "Epoch 8/10, Batch 500/1997, Loss: 0.1018\n",
      "Epoch 8/10, Batch 550/1997, Loss: 0.4098\n",
      "Epoch 8/10, Batch 600/1997, Loss: 0.1673\n",
      "Epoch 8/10, Batch 650/1997, Loss: 0.4308\n",
      "Epoch 8/10, Batch 700/1997, Loss: 0.2277\n",
      "Epoch 8/10, Batch 750/1997, Loss: 0.1885\n",
      "Epoch 8/10, Batch 800/1997, Loss: 0.1762\n",
      "Epoch 8/10, Batch 850/1997, Loss: 0.1820\n",
      "Epoch 8/10, Batch 900/1997, Loss: 0.2906\n",
      "Epoch 8/10, Batch 950/1997, Loss: 0.1499\n",
      "Epoch 8/10, Batch 1000/1997, Loss: 0.2128\n",
      "Epoch 8/10, Batch 1050/1997, Loss: 0.2496\n",
      "Epoch 8/10, Batch 1100/1997, Loss: 0.2925\n",
      "Epoch 8/10, Batch 1150/1997, Loss: 0.2398\n",
      "Epoch 8/10, Batch 1200/1997, Loss: 0.2158\n",
      "Epoch 8/10, Batch 1250/1997, Loss: 0.2464\n",
      "Epoch 8/10, Batch 1300/1997, Loss: 0.3114\n",
      "Epoch 8/10, Batch 1350/1997, Loss: 0.3591\n",
      "Epoch 8/10, Batch 1400/1997, Loss: 0.2409\n",
      "Epoch 8/10, Batch 1450/1997, Loss: 0.1789\n",
      "Epoch 8/10, Batch 1500/1997, Loss: 0.3836\n",
      "Epoch 8/10, Batch 1550/1997, Loss: 0.2116\n",
      "Epoch 8/10, Batch 1600/1997, Loss: 0.4605\n",
      "Epoch 8/10, Batch 1650/1997, Loss: 0.1937\n",
      "Epoch 8/10, Batch 1700/1997, Loss: 0.1755\n",
      "Epoch 8/10, Batch 1750/1997, Loss: 0.1886\n",
      "Epoch 8/10, Batch 1800/1997, Loss: 0.2194\n",
      "Epoch 8/10, Batch 1850/1997, Loss: 0.3529\n",
      "Epoch 8/10, Batch 1900/1997, Loss: 0.3442\n",
      "Epoch 8/10, Batch 1950/1997, Loss: 0.2987\n",
      "Epoch 8: Train Loss: 0.2723, Val Acc: 0.8922\n",
      "Epoch 9/10, Batch 0/1997, Loss: 0.2814\n",
      "Epoch 9/10, Batch 50/1997, Loss: 0.2503\n",
      "Epoch 9/10, Batch 100/1997, Loss: 0.3853\n",
      "Epoch 9/10, Batch 150/1997, Loss: 0.1391\n",
      "Epoch 9/10, Batch 200/1997, Loss: 0.2821\n",
      "Epoch 9/10, Batch 250/1997, Loss: 0.2419\n",
      "Epoch 9/10, Batch 300/1997, Loss: 0.2025\n",
      "Epoch 9/10, Batch 350/1997, Loss: 0.3107\n",
      "Epoch 9/10, Batch 400/1997, Loss: 0.1824\n",
      "Epoch 9/10, Batch 450/1997, Loss: 0.1703\n",
      "Epoch 9/10, Batch 500/1997, Loss: 0.2540\n",
      "Epoch 9/10, Batch 550/1997, Loss: 0.3812\n",
      "Epoch 9/10, Batch 600/1997, Loss: 0.3496\n",
      "Epoch 9/10, Batch 650/1997, Loss: 0.3571\n",
      "Epoch 9/10, Batch 700/1997, Loss: 0.5668\n",
      "Epoch 9/10, Batch 750/1997, Loss: 0.1337\n",
      "Epoch 9/10, Batch 800/1997, Loss: 0.1398\n",
      "Epoch 9/10, Batch 850/1997, Loss: 0.2619\n",
      "Epoch 9/10, Batch 900/1997, Loss: 0.1776\n",
      "Epoch 9/10, Batch 950/1997, Loss: 0.2032\n",
      "Epoch 9/10, Batch 1000/1997, Loss: 0.3802\n",
      "Epoch 9/10, Batch 1050/1997, Loss: 0.3201\n",
      "Epoch 9/10, Batch 1100/1997, Loss: 0.2428\n",
      "Epoch 9/10, Batch 1150/1997, Loss: 0.3280\n",
      "Epoch 9/10, Batch 1200/1997, Loss: 0.3548\n",
      "Epoch 9/10, Batch 1250/1997, Loss: 0.2369\n",
      "Epoch 9/10, Batch 1300/1997, Loss: 0.2529\n",
      "Epoch 9/10, Batch 1350/1997, Loss: 0.4659\n",
      "Epoch 9/10, Batch 1400/1997, Loss: 0.2057\n",
      "Epoch 9/10, Batch 1450/1997, Loss: 0.1399\n",
      "Epoch 9/10, Batch 1500/1997, Loss: 0.1954\n",
      "Epoch 9/10, Batch 1550/1997, Loss: 0.2839\n",
      "Epoch 9/10, Batch 1600/1997, Loss: 0.3686\n",
      "Epoch 9/10, Batch 1650/1997, Loss: 0.3159\n",
      "Epoch 9/10, Batch 1700/1997, Loss: 0.2622\n",
      "Epoch 9/10, Batch 1750/1997, Loss: 0.4819\n",
      "Epoch 9/10, Batch 1800/1997, Loss: 0.1366\n",
      "Epoch 9/10, Batch 1850/1997, Loss: 0.4178\n",
      "Epoch 9/10, Batch 1900/1997, Loss: 0.2554\n",
      "Epoch 9/10, Batch 1950/1997, Loss: 0.3195\n",
      "Epoch 9: Train Loss: 0.2594, Val Acc: 0.8902\n",
      "Epoch 10/10, Batch 0/1997, Loss: 0.3097\n",
      "Epoch 10/10, Batch 50/1997, Loss: 0.2397\n",
      "Epoch 10/10, Batch 100/1997, Loss: 0.1921\n",
      "Epoch 10/10, Batch 150/1997, Loss: 0.1976\n",
      "Epoch 10/10, Batch 200/1997, Loss: 0.1496\n",
      "Epoch 10/10, Batch 250/1997, Loss: 0.1969\n",
      "Epoch 10/10, Batch 300/1997, Loss: 0.4122\n",
      "Epoch 10/10, Batch 350/1997, Loss: 0.1598\n",
      "Epoch 10/10, Batch 400/1997, Loss: 0.2297\n",
      "Epoch 10/10, Batch 450/1997, Loss: 0.2283\n",
      "Epoch 10/10, Batch 500/1997, Loss: 0.3577\n",
      "Epoch 10/10, Batch 550/1997, Loss: 0.2077\n",
      "Epoch 10/10, Batch 600/1997, Loss: 0.2923\n",
      "Epoch 10/10, Batch 650/1997, Loss: 0.0955\n",
      "Epoch 10/10, Batch 700/1997, Loss: 0.1678\n",
      "Epoch 10/10, Batch 750/1997, Loss: 0.5932\n",
      "Epoch 10/10, Batch 800/1997, Loss: 0.2423\n",
      "Epoch 10/10, Batch 850/1997, Loss: 0.2952\n",
      "Epoch 10/10, Batch 900/1997, Loss: 0.2843\n",
      "Epoch 10/10, Batch 950/1997, Loss: 0.2777\n",
      "Epoch 10/10, Batch 1000/1997, Loss: 0.2034\n",
      "Epoch 10/10, Batch 1050/1997, Loss: 0.3384\n",
      "Epoch 10/10, Batch 1100/1997, Loss: 0.1801\n",
      "Epoch 10/10, Batch 1150/1997, Loss: 0.2235\n",
      "Epoch 10/10, Batch 1200/1997, Loss: 0.2005\n",
      "Epoch 10/10, Batch 1250/1997, Loss: 0.2892\n",
      "Epoch 10/10, Batch 1300/1997, Loss: 0.1918\n",
      "Epoch 10/10, Batch 1350/1997, Loss: 0.3931\n",
      "Epoch 10/10, Batch 1400/1997, Loss: 0.2521\n",
      "Epoch 10/10, Batch 1450/1997, Loss: 0.2791\n",
      "Epoch 10/10, Batch 1500/1997, Loss: 0.1645\n",
      "Epoch 10/10, Batch 1550/1997, Loss: 0.2623\n",
      "Epoch 10/10, Batch 1600/1997, Loss: 0.2920\n",
      "Epoch 10/10, Batch 1650/1997, Loss: 0.4525\n",
      "Epoch 10/10, Batch 1700/1997, Loss: 0.1832\n",
      "Epoch 10/10, Batch 1750/1997, Loss: 0.2416\n",
      "Epoch 10/10, Batch 1800/1997, Loss: 0.3281\n",
      "Epoch 10/10, Batch 1850/1997, Loss: 0.1572\n",
      "Epoch 10/10, Batch 1900/1997, Loss: 0.2193\n",
      "Epoch 10/10, Batch 1950/1997, Loss: 0.1375\n",
      "Epoch 10: Train Loss: 0.2453, Val Acc: 0.8895\n",
      "Batch 1/624\n",
      "Batch 21/624\n",
      "Batch 41/624\n",
      "Batch 61/624\n",
      "Batch 81/624\n",
      "Batch 101/624\n",
      "Batch 121/624\n",
      "Batch 141/624\n",
      "Batch 161/624\n",
      "Batch 181/624\n",
      "Batch 201/624\n",
      "Batch 221/624\n",
      "Batch 241/624\n",
      "Batch 261/624\n",
      "Batch 281/624\n",
      "Batch 301/624\n",
      "Batch 321/624\n",
      "Batch 341/624\n",
      "Batch 361/624\n",
      "Batch 381/624\n",
      "Batch 401/624\n",
      "Batch 421/624\n",
      "Batch 441/624\n",
      "Batch 461/624\n",
      "Batch 481/624\n",
      "Batch 501/624\n",
      "Batch 521/624\n",
      "Batch 541/624\n",
      "Batch 561/624\n",
      "Batch 581/624\n",
      "Batch 601/624\n",
      "Batch 621/624\n",
      "🏃 View run CPU_Quick_BiLSTM_units_64_dropout_0.5 at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/375501789462796807/runs/871ace1f056e46d380d3be202beffa8c\n",
      "🧪 View experiment at: http://ec2-13-204-157-189.ap-south-1.compute.amazonaws.com:5000/#/experiments/375501789462796807\n"
     ]
    }
   ],
   "source": [
    "def run_quick_experiment_pytorch_lstm(hidden_units, dropout_rate, bidirectional=False):\n",
    "    \"\"\"\n",
    "    CPU-optimized version for quick experimentation\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device('cpu')  # Force CPU\n",
    "    \n",
    "    # REDUCED PARAMETERS FOR CPU\n",
    "    max_features = 5000   # Reduced from 10,000\n",
    "    maxlen = 50          # Reduced from 100\n",
    "    embedding_dim = 64   # Reduced from 128\n",
    "    batch_size = 64      # Increased for efficiency\n",
    "    epochs = 10          # Reduced from 50\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    print(f\"Starting experiment: {hidden_units} units, dropout={dropout_rate}, bidirectional={bidirectional}\")\n",
    "    \n",
    "    # Split the data (clean NaN values first)\n",
    "    df_clean = df.dropna(subset=['clean_text', 'category'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_clean['clean_text'], df_clean['category'], \n",
    "        test_size=0.2, random_state=42, stratify=df_clean['category']\n",
    "    )\n",
    "    \n",
    "    # Further split training into train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokenizer = SimpleTokenizer(max_features=max_features)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(X_train, y_train_encoded, tokenizer, maxlen)\n",
    "    val_dataset = TextDataset(X_val, y_val_encoded, tokenizer, maxlen)\n",
    "    test_dataset = TextDataset(X_test, y_test_encoded, tokenizer, maxlen)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        lstm_type = \"BiLSTM\" if bidirectional else \"LSTM\"\n",
    "        mlflow.set_tag(\"mlflow.runName\", f\"CPU_Quick_{lstm_type}_units_{hidden_units}_dropout_{dropout_rate}\")\n",
    "        mlflow.set_tag(\"experiment_type\", \"pytorch_cpu_quick\")\n",
    "        mlflow.set_tag(\"model_type\", f\"PyTorch{lstm_type}Classifier\")\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"device\", \"CPU\")\n",
    "        mlflow.log_param(\"max_features\", max_features)\n",
    "        mlflow.log_param(\"maxlen\", maxlen)\n",
    "        mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "        mlflow.log_param(\"lstm_units\", hidden_units)\n",
    "        mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "        mlflow.log_param(\"bidirectional\", bidirectional)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SentimentLSTM(\n",
    "            vocab_size=len(tokenizer.word_to_index),\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_units,\n",
    "            num_layers=1,  # Single layer for CPU\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            bidirectional=bidirectional\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Training with progress\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, (batch_texts, batch_labels) in enumerate(train_loader):\n",
    "                batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_texts)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Progress indicator\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_texts, batch_labels in val_loader:\n",
    "                    batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                    outputs = model(batch_texts)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += batch_labels.size(0)\n",
    "                    val_correct += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            val_acc = val_correct / val_total\n",
    "            train_losses.append(total_loss / len(train_loader))\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_losses[-1]:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (batch_texts, batch_labels) in enumerate(test_loader):\n",
    "                # Progress indicator\n",
    "                if batch_idx % 20 == 0:\n",
    "                    print(f\"Batch {batch_idx + 1}/{len(test_loader)}\")\n",
    "                \n",
    "                batch_texts = batch_texts.to(device, non_blocking=True)\n",
    "                batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "                \n",
    "                outputs = model(batch_texts)\n",
    "                predicted = torch.argmax(outputs, dim=1)  # More efficient than torch.max\n",
    "                \n",
    "                test_total += batch_labels.size(0)\n",
    "                test_correct += (predicted == batch_labels).sum().item()\n",
    "                \n",
    "                # Clear variables to prevent memory buildup\n",
    "                del outputs, predicted\n",
    "                \n",
    "                # Periodic memory cleanup\n",
    "                if batch_idx % 50 == 0:\n",
    "                    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        # Quick CPU experiments - should run in 30-45 minutes total\n",
    "quick_configs = [\n",
    "    {\"hidden_units\": 128, \"dropout_rate\": 0.3, \"bidirectional\": False},\n",
    "    {\"hidden_units\": 64, \"dropout_rate\": 0.5, \"bidirectional\": True},\n",
    "]\n",
    "\n",
    "# Run quick experiments\n",
    "for config in quick_configs:\n",
    "    run_quick_experiment_pytorch_lstm(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0f474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
